{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKSTLF2BX6jH"
   },
   "source": [
    "### Ivy(Yacheng) Qiu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N11Ee3GJmywu",
    "outputId": "64e10b54-dad3-48e2-c266-617670a17e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /Users/qyc/anaconda3/lib/python3.10/site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: sniffio in /Users/qyc/anaconda3/lib/python3.10/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/qyc/anaconda3/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>4 in /Users/qyc/anaconda3/lib/python3.10/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/qyc/anaconda3/lib/python3.10/site-packages (from openai) (1.10.13)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/qyc/anaconda3/lib/python3.10/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/qyc/anaconda3/lib/python3.10/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/qyc/anaconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/qyc/anaconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /Users/qyc/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/qyc/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/qyc/anaconda3/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=9d0f597606e7986ef69377221e1d437454bdc71f5dcf5a94d81b74bb2abbd2ca\n",
      "  Stored in directory: /Users/qyc/Library/Caches/pip/wheels/b2/7f/26/524faff9145e274da278dc97d63ab0bfde1f791ecf101a9c95\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: h11, distro, wikipedia, httpcore, httpx, openai\n",
      "Successfully installed distro-1.9.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0 wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Q2A8TGhKm3i5"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E9HEMJSX-3T"
   },
   "source": [
    "# 1.) Set up OpenAI and the enviornment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4zwwdkZDYDZN"
   },
   "outputs": [],
   "source": [
    "apikey = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8IiKS0snlpYP"
   },
   "outputs": [],
   "source": [
    "openai.api_key = apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "        api_key = apikey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOXc5_BTm9HP"
   },
   "source": [
    "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-v7OYamHlrEB"
   },
   "outputs": [],
   "source": [
    "page_titles = [\"Artificial Intelligence\", \"UCLA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TgY2FkTdmhTH"
   },
   "outputs": [],
   "source": [
    "page_title = page_titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Kw5H5jMlmmS3"
   },
   "outputs": [],
   "source": [
    "search_results = wikipedia.search(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZF3BiZyXltYO"
   },
   "outputs": [],
   "source": [
    "page = wikipedia.page(search_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ef7yfa2jl0iZ"
   },
   "outputs": [],
   "source": [
    "#page. content \n",
    "def get_wikipedia_content(page_title):\n",
    "    search_results = wikipedia.search(page_title)\n",
    "    page = wikipedia.page(search_results[0])\n",
    "    return(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = get_wikipedia_content(page_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9aruncMmubX"
   },
   "source": [
    "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Bmai3B6Dmw3O"
   },
   "outputs": [],
   "source": [
    "chat_completions = client.chat.completions.create(\n",
    "    model = \"gpt-4\",\n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : \"You are a summary assistant at wikipedia, I will pass you an article. Please concisely list only the fa;se information found. If nothing is false, say Done\"}, \n",
    "        {\"role\" : \"user\", \"content\" : content[:8180]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1jI-un5PnDjg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completions.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_TMKFGN4nDJ4"
   },
   "outputs": [],
   "source": [
    "def chatgpt_error_correction(text): \n",
    "    chat_completions = client.chat.completions.create(\n",
    "    model = \"gpt-4\",\n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : \"You are a summary assistant at wikipedia, I will pass you an article. Please concisely list only the false information found. If nothing is false, say Done\"}, \n",
    "        {\"role\" : \"user\", \"content\" : content[:8180]}])\n",
    "    print(chat_completions.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FKAJVXSoayA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPw5LyPEobmk"
   },
   "source": [
    "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "V7cuhML2ocGn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________Artificial Intelligence\n",
      "Done\n",
      "_________UCLA\n",
      "1. UCLA was originally not established as the southern branch of the California State Normal School that evolved into San José State University. It originated as the Los Angeles branch of the California Normal School.\n",
      "2. UCLA didn't receive 174,914 undergraduate applications for Fall 2022, the actual numbers are not mentioned in the document.\n",
      "3. The California State Normal School branch did not open on August 29, 1882. It opened on August 8, 1882.\n",
      "4. The information regarding David Prescott Barrows being the President of the University of California in 1919 is incorrect. He was president from 1919 to 1923. \n",
      "5. The Southern Branch campus did not open on September 15, 1919. It opened on September 9, 1919.\n",
      "6. The name change from the Southern Branch to the University of California at Los Angeles happened in 1927, not on February 1, 1927.\n",
      "7. UCLA was not permitted to award the doctorate in 1936. The university began to award doctorates in a variety of disciplines from 1930 onwards.\n",
      "8. UCLA was not treated as an off-site department of the main campus in Berkeley for its first 32 years. It became a comprehensive university much sooner.\n",
      "9. The murder-suicide at an engineering building at the university did not occur on June 1, 2016. It happened on June 2, 2016.\n",
      "10. The graduate student adviser and professor in the history department Gabriel Piterberg was not accused of sexually assaulting two students in 2014, the accusations emerged in years prior.\n"
     ]
    }
   ],
   "source": [
    "for page_title in page_titles:\n",
    "    try:\n",
    "        print(\"_________\" + page_title)\n",
    "        content = get_wikipedia_content(page_title)\n",
    "        chatgpt_error_correction(content)\n",
    "    except:\n",
    "        print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
